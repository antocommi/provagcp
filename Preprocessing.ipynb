{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOaBGNVZY2oZN1BU4PBcjQz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antocommi/provagcp/blob/master/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "BlGCDVCDi8Jr",
        "outputId": "c52cca64-1ad4-400a-8753-40dabfe0a000"
      },
      "source": [
        "SEED = 9126\n",
        "\n",
        "import os, cv2, json, time, math, sys, pickle, collections\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(SEED)\n",
        "import seaborn as sn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "USE_CUDA = True\n",
        "CUDA = USE_CUDA and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
        "if CUDA:\n",
        "    print('run on %s' % device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5de144c30701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    259\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnCzEg86i9sK"
      },
      "source": [
        "# Download and load in memory dataset from kaggle\n",
        "!mkdir ~/.kaggle #create the .kaggle folder in your root directory\n",
        "!echo '{\"username\":\"antocommii\",\"key\":\"87dcebd7c4cb4ba4539ed72f027fcbde\"}' > ~/.kaggle/kaggle.json #write kaggle API credentials to kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json  # set permissions\n",
        "!kaggle datasets download --unzip --force antocommii/spacejam-action-recognition -p /content/kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qe5utRLihkU"
      },
      "source": [
        "\n",
        "\n",
        "class CustomSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, videos_dir, list_dataset, transform):\n",
        "        \"\"\" Construct an indexed list of video paths and labels \"\"\"\n",
        "        self.transform = transform\n",
        "        self.VIDEO_DIR = videos_dir\n",
        "        \n",
        "        self.labels_dict = {0 : \"block\", 1 : \"pass\", 2 : \"run\", 3: \"dribble\",4: \"shoot\",\n",
        "          5 : \"ball in hand\", 6 : \"defense\", 7: \"pick\" , 8 : \"no_action\" , \n",
        "          9: \"walk\" ,10: \"discard\"}\n",
        "\n",
        "        self.dataset = list_dataset\n",
        "\n",
        "    def __getitem__(self, index, is_for_testing=True):\n",
        "        \"\"\" Load video n in the list of image paths and return it along with its label.\n",
        "            In the case of multiclass the label will probably be a list of values\"\"\"\n",
        "        \n",
        "        name, label = self.dataset[index]\n",
        "        \n",
        "        fname = os.path.join(self.VIDEO_DIR, name+'.mp4')\n",
        "        \n",
        "        video = []\n",
        "        vid = cv2.VideoCapture(fname)\n",
        "        while True:\n",
        "            # Capture frame-by-frame\n",
        "            ret, frame = vid.read()\n",
        "            if ret != True:\n",
        "              break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = self.transform(Image.fromarray(frame))\n",
        "            #frame = frame.unsqueeze(0)\n",
        "            video.append(frame)\n",
        "\n",
        "        video = torch.stack(video)\n",
        "        video = torch.transpose(video, 1, 0)\n",
        "        return {'video':video, 'label':torch.LongTensor([label])}\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" return the total number of video in this dataset \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def get_list(self):\n",
        "      return self.dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "177wjqEfisK5"
      },
      "source": [
        "new_H, new_W = 150,120\n",
        "print(\"New Size: \",new_H,new_W)\n",
        "mean = [0.485,0.456,0.406]\n",
        "std = [0.229,0.224,0.225]\n",
        "train_transform = transforms.Compose([\n",
        "                                        # transforms.ColorJitter(hue=.5, saturation=.5,brightness=.5),\n",
        "                                        transforms.CenterCrop((new_H,new_W)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean,std)\n",
        "                                      ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "                                        transforms.CenterCrop((new_H,new_W)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean,std)\n",
        "                                     ])\n",
        "\n",
        "\n",
        "# abbiamo 10 classi\n",
        "num_classes = 10\n",
        "\n",
        "# il dataset è ciò che si ottiene dal metodo precedente\n",
        "ROOT_DIR = \"/content/kaggle/\"\n",
        "VIDEO_DIR = \"/content/kaggle/examples/\"\n",
        "ANNOTATION_FILE = \"annotation_dict.json\"\n",
        "TEST_SET_FILE = \"testset_keys_1lug2020.txt\"\n",
        "\n",
        "# prendo il dataset da annotation file\n",
        "with open(os.path.join(ROOT_DIR, ANNOTATION_FILE)) as fp:\n",
        "  annotations = json.load(fp)\n",
        "            \n",
        "with open(os.path.join(ROOT_DIR, TEST_SET_FILE)) as fp:\n",
        "  keys_test = json.load(fp)\n",
        "\n",
        "# divido in train e test second quello che ho nel file\n",
        "annotationTrain = dict(filter(lambda x: x[0] not in keys_test, annotations.items()))\n",
        "annotationTest = dict(filter(lambda x: x[0] in keys_test, annotations.items()))\n",
        "test_set = list(annotationTest.items())\n",
        "\n",
        "# divido in train e validation\n",
        "test_size = 0.2\n",
        "train_set, valid_set = train_test_split(list(annotationTrain.items()), random_state=456, test_size=test_size, stratify=list(annotationTrain.values()))\n",
        "\n",
        "# trasformo in custom_dataset\n",
        "train_ds = CustomSet(VIDEO_DIR, train_set, train_transform)\n",
        "valid_ds = CustomSet(VIDEO_DIR, valid_set, test_transform)\n",
        "test_ds = CustomSet(VIDEO_DIR, test_set, test_transform)\n",
        "\n",
        "# definisco le batch_size\n",
        "BATCH_TRAIN_SIZE, BATCH_TEST_SIZE = 16,16\n",
        "\n",
        "# creo i dataloader\n",
        "trainLoader = DataLoader(train_ds, batch_size=BATCH_TRAIN_SIZE, shuffle=True, num_)\n",
        "validLoader = DataLoader(valid_ds, batch_size=BATCH_TEST_SIZE, shuffle=True)\n",
        "testLoader = DataLoader(test_ds, batch_size=BATCH_TEST_SIZE, shuffle=True)\n",
        "\n",
        "# controllo se hanno la stessa forma di lista di tuple\n",
        "print(len(trainLoader), len(validLoader), len(testLoader))\n",
        "print(train_set[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2QGwRrhizEv"
      },
      "source": [
        "labels_dict = {0 : \"block\", 1 : \"pass\", 2 : \"run\", 3: \"dribble\",4: \"shoot\",\n",
        "          5 : \"ball in hand\", 6 : \"defense\", 7: \"pick\" , 8 : \"no_action\" , \n",
        "          9: \"walk\" ,10: \"discard\"}\n",
        "\n",
        "for item in trainLoader:\n",
        "  video = item['video'][0]\n",
        "  label = item['label'][0]\n",
        "  break\n",
        "clip = torch.transpose(video, 0, 1)\n",
        "\n",
        "print(labels_dict[label.item()])\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i in range(clip.shape[0]):\n",
        "  plt.subplot(4,4, i+1)\n",
        "  video = clip[i]\n",
        "  trans = transforms.ToPILImage(mode=\"RGB\")\n",
        "  video = trans(video)\n",
        "  plt.imshow(video)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}