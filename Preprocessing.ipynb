{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPL2l+OubP8yEfQW0XFQHm+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antocommi/provagcp/blob/master/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlGCDVCDi8Jr"
      },
      "source": [
        "SEED = 9126\n",
        "\n",
        "import os, cv2, json, time, math, sys, pickle, collections\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(SEED)\n",
        "import seaborn as sn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "USE_CUDA = True\n",
        "CUDA = USE_CUDA and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
        "if CUDA:\n",
        "    print('run on %s' % device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnCzEg86i9sK"
      },
      "source": [
        "# Download and load in memory dataset from kaggle\n",
        "!mkdir ~/.kaggle #create the .kaggle folder in your root directory\n",
        "!echo '{\"username\":\"antocommii\",\"key\":\"87dcebd7c4cb4ba4539ed72f027fcbde\"}' > ~/.kaggle/kaggle.json #write kaggle API credentials to kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json  # set permissions\n",
        "!kaggle datasets download --unzip --force antocommii/spacejam-action-recognition -p /content/kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qe5utRLihkU"
      },
      "source": [
        "\n",
        "\n",
        "class CustomSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, videos_dir, list_dataset, transform):\n",
        "        \"\"\" Construct an indexed list of video paths and labels \"\"\"\n",
        "        self.transform = transform\n",
        "        self.VIDEO_DIR = videos_dir\n",
        "        \n",
        "        self.labels_dict = {0 : \"block\", 1 : \"pass\", 2 : \"run\", 3: \"dribble\",4: \"shoot\",\n",
        "          5 : \"ball in hand\", 6 : \"defense\", 7: \"pick\" , 8 : \"no_action\" , \n",
        "          9: \"walk\" ,10: \"discard\"}\n",
        "\n",
        "        self.dataset = list_dataset\n",
        "\n",
        "    def __getitem__(self, index, is_for_testing=True):\n",
        "        \"\"\" Load video n in the list of image paths and return it along with its label.\n",
        "            In the case of multiclass the label will probably be a list of values\"\"\"\n",
        "        \n",
        "        name, label = self.dataset[index]\n",
        "        \n",
        "        fname = os.path.join(self.VIDEO_DIR, name+'.mp4')\n",
        "        \n",
        "        video = []\n",
        "        vid = cv2.VideoCapture(fname)\n",
        "        while True:\n",
        "            # Capture frame-by-frame\n",
        "            ret, frame = vid.read()\n",
        "            if ret != True:\n",
        "              break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = self.transform(Image.fromarray(frame))\n",
        "            #frame = frame.unsqueeze(0)\n",
        "            video.append(frame)\n",
        "\n",
        "        video = torch.stack(video)\n",
        "        video = torch.transpose(video, 1, 0)\n",
        "        return {'video':video, 'label':torch.LongTensor([label])}\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" return the total number of video in this dataset \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def get_list(self):\n",
        "      return self.dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "177wjqEfisK5"
      },
      "source": [
        "new_H, new_W = 150,120\n",
        "print(\"New Size: \",new_H,new_W)\n",
        "mean = [0.485,0.456,0.406]\n",
        "std = [0.229,0.224,0.225]\n",
        "train_transform = transforms.Compose([\n",
        "                                        # transforms.ColorJitter(hue=.5, saturation=.5,brightness=.5),\n",
        "                                        transforms.CenterCrop((new_H,new_W)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean,std)\n",
        "                                      ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "                                        transforms.CenterCrop((new_H,new_W)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean,std)\n",
        "                                     ])\n",
        "\n",
        "\n",
        "# abbiamo 10 classi\n",
        "num_classes = 10\n",
        "\n",
        "# il dataset è ciò che si ottiene dal metodo precedente\n",
        "ROOT_DIR = \"/content/kaggle/\"\n",
        "VIDEO_DIR = \"/content/kaggle/examples/\"\n",
        "ANNOTATION_FILE = \"annotation_dict.json\"\n",
        "TEST_SET_FILE = \"testset_keys_1lug2020.txt\"\n",
        "\n",
        "# prendo il dataset da annotation file\n",
        "with open(os.path.join(ROOT_DIR, ANNOTATION_FILE)) as fp:\n",
        "  annotations = json.load(fp)\n",
        "            \n",
        "with open(os.path.join(ROOT_DIR, TEST_SET_FILE)) as fp:\n",
        "  keys_test = json.load(fp)\n",
        "\n",
        "# divido in train e test second quello che ho nel file\n",
        "annotationTrain = dict(filter(lambda x: x[0] not in keys_test, annotations.items()))\n",
        "annotationTest = dict(filter(lambda x: x[0] in keys_test, annotations.items()))\n",
        "test_set = list(annotationTest.items())\n",
        "\n",
        "# divido in train e validation\n",
        "test_size = 0.2\n",
        "train_set, valid_set = train_test_split(list(annotationTrain.items()), random_state=456, test_size=test_size, stratify=list(annotationTrain.values()))\n",
        "\n",
        "# trasformo in custom_dataset\n",
        "train_ds = CustomSet(VIDEO_DIR, train_set, train_transform)\n",
        "valid_ds = CustomSet(VIDEO_DIR, valid_set, test_transform)\n",
        "test_ds = CustomSet(VIDEO_DIR, test_set, test_transform)\n",
        "\n",
        "# definisco le batch_size\n",
        "BATCH_TRAIN_SIZE, BATCH_TEST_SIZE = 16,16\n",
        "\n",
        "# creo i dataloader\n",
        "trainLoader = DataLoader(train_ds, batch_size=BATCH_TRAIN_SIZE, shuffle=True)\n",
        "validLoader = DataLoader(valid_ds, batch_size=BATCH_TEST_SIZE, shuffle=True)\n",
        "testLoader = DataLoader(test_ds, batch_size=BATCH_TEST_SIZE, shuffle=True)\n",
        "\n",
        "# controllo se hanno la stessa forma di lista di tuple\n",
        "print(len(trainLoader), len(validLoader), len(testLoader))\n",
        "print(train_set[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2QGwRrhizEv"
      },
      "source": [
        "labels_dict = {0 : \"block\", 1 : \"pass\", 2 : \"run\", 3: \"dribble\",4: \"shoot\",\n",
        "          5 : \"ball in hand\", 6 : \"defense\", 7: \"pick\" , 8 : \"no_action\" , \n",
        "          9: \"walk\" ,10: \"discard\"}\n",
        "\n",
        "for item in trainLoader:\n",
        "  video = item['video'][0]\n",
        "  label = item['label'][0]\n",
        "  break\n",
        "clip = torch.transpose(video, 0, 1)\n",
        "\n",
        "print(labels_dict[label.item()])\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i in range(clip.shape[0]):\n",
        "  plt.subplot(4,4, i+1)\n",
        "  video = clip[i]\n",
        "  trans = transforms.ToPILImage(mode=\"RGB\")\n",
        "  video = trans(video)\n",
        "  plt.imshow(video)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}