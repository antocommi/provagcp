{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di ProgettoAIM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "t68MYk9fukmZ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antocommi/provagcp/blob/master/test_s3d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPOa60j-cQ73"
      },
      "source": [
        "Required modules \r\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RHLRE_rdwjr"
      },
      "source": [
        "# # DOPO AVER ESEGUITO QUESTA CELLA RIAVVIARE IL RUNTIME!\n",
        "\n",
        "!pip install -U torch torchvision cython\n",
        "!pip install -U 'git+https://github.com/facebookresearch/fvcore.git' 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "!git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n",
        "!pip install -e detectron2_repo\n",
        "!pip install wget \n",
        "\n",
        "!git clone https://github.com/facebookresearch/slowfast\n",
        "\n",
        "%cd slowfast\n",
        "!python setup.py build develop\n",
        "%cd ..\n",
        "!export PYTHONPATH=/path/to/SlowFast/slowfast:$PYTHONPATH\n",
        "\n",
        "# # DOPO AVER ESEGUITO QUESTA CELLA RIAVVIARE IL RUNTIME!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShTSrpSncN4p"
      },
      "source": [
        "from slowfast.config.defaults import get_cfg\r\n",
        "from slowfast.models import build_model\r\n",
        "from slowfast.utils.checkpoint import load_checkpoint\r\n",
        "import wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq8uD48_c4KD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd8c4ca6-574b-4a9d-cf48-62a6e097a26a"
      },
      "source": [
        "!pip install -q -U albumentations "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 81kB 8.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.6MB 80kB/s \n",
            "\u001b[K     |████████████████████████████████| 952kB 51.8MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7toodb0ceWQ"
      },
      "source": [
        "Dependencies\r\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mecebytbcLrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c93c79-8f9e-411d-ba8f-3797ed75120e"
      },
      "source": [
        "SEED = 9126\n",
        "\n",
        "import os, cv2, json, time, math, sys, pickle, collections, PIL\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import albumentations as A\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(SEED)\n",
        "import seaborn as sn\n",
        "\n",
        "#from vmz import models\n",
        "#import mmaction\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "USE_CUDA = True\n",
        "CUDA = USE_CUDA and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
        "if CUDA:\n",
        "    print('run on %s' % device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "run on cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z3LYU1fmiXE"
      },
      "source": [
        "\n",
        "Importing from Kaggle\n",
        "===="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENNsCSGCz4KY",
        "outputId": "805c1286-a741-42b8-bf7a-6c6f6624a39c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download and load in memory dataset from kaggle\n",
        "!mkdir ~/.kaggle #create the .kaggle folder in your root directory\n",
        "!echo '{\"username\":\"antocommii\",\"key\":\"87dcebd7c4cb4ba4539ed72f027fcbde\"}' > ~/.kaggle/kaggle.json #write kaggle API credentials to kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json  # set permissions\n",
        "!kaggle datasets download --unzip --force antocommii/spacejam-action-recognition -p /content/kaggle/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading spacejam-action-recognition.zip to /content/kaggle\n",
            " 96% 604M/626M [00:04<00:00, 91.8MB/s]\n",
            "100% 626M/626M [00:04<00:00, 135MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIzdgV3ecyxy"
      },
      "source": [
        "# il dataset è ciò che si ottiene dal metodo precedente\n",
        "ROOT_DIR = \"/content/kaggle/\"\n",
        "VIDEO_DIR = \"/content/kaggle/examples/\"\n",
        "ANNOTATION_FILE = \"annotation_dict.json\"\n",
        "TEST_SET_FILE = \"testset_keys_1lug2020.txt\"\n",
        "\n",
        "# prendo il dataset da annotation file\n",
        "with open(os.path.join(ROOT_DIR, ANNOTATION_FILE)) as fp:\n",
        "  annotations = json.load(fp)\n",
        "            \n",
        "with open(os.path.join(ROOT_DIR, TEST_SET_FILE)) as fp:\n",
        "  keys_test = json.load(fp)\n",
        "\n",
        "# divido in train e test second quello che ho nel file\n",
        "annotationTrain = dict(filter(lambda x: x[0] not in keys_test, annotations.items()))\n",
        "annotationTest = dict(filter(lambda x: x[0] in keys_test, annotations.items()))\n",
        "test_set = list(annotationTest.items())\n",
        "\n",
        "# divido in train e validation\n",
        "test_size = 0.2\n",
        "train_set, valid_set = train_test_split(list(annotationTrain.items()), random_state=456, test_size=test_size, stratify=list(annotationTrain.values()))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kOOsPYPmlch"
      },
      "source": [
        "Scelgo nuova size e definisco la struttura del Custom Dataset\n",
        "===="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e5I2kBycS9D"
      },
      "source": [
        "class CustomSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, videos_dir, list_dataset, transform):\n",
        "        \"\"\" Construct an indexed list of video paths and labels \"\"\"\n",
        "        self.transform = transform\n",
        "        self.VIDEO_DIR = videos_dir\n",
        "        self.dataset = list_dataset\n",
        "        # # pytorch zoo & moabitcoin \n",
        "        # mean = [0.43216, 0.394666, 0.37645]\n",
        "        # std = [0.22803, 0.22145, 0.216989]\n",
        "        # # kinetics\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "\n",
        "        self.toTensor = transforms.Compose([\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, index, is_for_testing=True):\n",
        "        \"\"\" Load video n in the list of image paths and return it along with its label.\n",
        "            In the case of multiclass the label will probably be a list of values\"\"\"\n",
        "        \n",
        "        name, label = self.dataset[index]\n",
        "        \n",
        "        fname = os.path.join(self.VIDEO_DIR, name+'.mp4')\n",
        "        \n",
        "        video = []\n",
        "        vid = cv2.VideoCapture(fname)\n",
        "        first = True\n",
        "        while True:\n",
        "            # Capture frame-by-frame\n",
        "            ret, frame = vid.read()\n",
        "            if ret != True:\n",
        "              break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            if first:\n",
        "              data = self.transform(image=np.array(frame))\n",
        "              frame = data['image']\n",
        "              replay = data['replay']\n",
        "              first = False\n",
        "            else: \n",
        "              frame = A.ReplayCompose.replay(replay, image=np.array(frame))['image']\n",
        "\n",
        "            frame = self.toTensor(Image.fromarray(frame))\n",
        "            video.append(frame)\n",
        "\n",
        "        video = torch.stack(video)\n",
        "        video = torch.transpose(video, 1, 0)\n",
        "        return {'video':video, 'label':torch.LongTensor([label])}\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" return the total number of video in this dataset \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def get_list(self):\n",
        "      return self.dataset\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khmMq62-mtba"
      },
      "source": [
        "Creazione Train, Validation e Test set\n",
        "===="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyvNusIhhr47",
        "outputId": "b73ab286-0dad-4b57-d4fc-b985bd04208e"
      },
      "source": [
        "crop_H, crop_W = 150,120\n",
        "new_H, new_W = 150,224\n",
        "print(\"New Size: \",new_H,new_W)\n",
        "print(\"Crop Size: \",176, 80)\n",
        "\n",
        "A_train = A.ReplayCompose([\n",
        "                     A.augmentations.transforms.CenterCrop(crop_H, crop_W, p=1),\n",
        "                    #  A.augmentations.Resize(new_H, new_H, p=1),\n",
        "                     A.HorizontalFlip(p=0.5),\n",
        "                     A.ShiftScaleRotate(scale_limit=(0,.4), rotate_limit=0.2,p=0.5),\n",
        "                     A.augmentations.transforms.CLAHE(clip_limit=4.0, p=.5)\n",
        "                     ])\n",
        "\n",
        "A_test = A.ReplayCompose([\n",
        "                     A.augmentations.transforms.CenterCrop(crop_H, crop_W, p=1),\n",
        "                    #  A.augmentations.Resize(new_H, new_H, p=1)\n",
        "                     ])\n",
        "\n",
        "\n",
        "# trasformo in custom_dataset\n",
        "train_ds = CustomSet(VIDEO_DIR, train_set, A_train)\n",
        "valid_ds = CustomSet(VIDEO_DIR, valid_set, A_test)\n",
        "test_ds = CustomSet(VIDEO_DIR, test_set, A_test)\n",
        "\n",
        "# definisco le batch_size\n",
        "BATCH_TRAIN_SIZE, BATCH_TEST_SIZE = 16,16\n",
        "\n",
        "# creo i dataloader\n",
        "trainLoader = DataLoader(train_ds, batch_size=BATCH_TRAIN_SIZE, shuffle=True, num_workers=4)\n",
        "validLoader = DataLoader(valid_ds, batch_size=BATCH_TEST_SIZE, shuffle=True, num_workers=4)\n",
        "testLoader = DataLoader(test_ds, batch_size=BATCH_TEST_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "# controllo se hanno la stessa forma di lista di tuple\n",
        "print(len(trainLoader), len(validLoader), len(testLoader))\n",
        "print(train_set[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New Size:  150 224\n",
            "Crop Size:  176 80\n",
            "1298 325 696\n",
            "('0011804', 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x-SdKzk0GRe"
      },
      "source": [
        "labels_dict = {0 : \"block\", 1 : \"pass\", 2 : \"run\", 3: \"dribble\",4: \"shoot\",\n",
        "          5 : \"ball in hand\", 6 : \"defense\", 7: \"pick\" , 8 : \"no_action\" , \n",
        "          9: \"walk\" ,10: \"discard\"}\n",
        "\n",
        "for item in trainLoader:\n",
        "  video = item['video'][0]\n",
        "  label = item['label'][0]\n",
        "  break\n",
        "clip = torch.transpose(video, 0, 1)\n",
        "\n",
        "print(labels_dict[label.item()])\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i in range(clip.shape[0]):\n",
        "  plt.subplot(4,4, i+1)\n",
        "  video = clip[i]\n",
        "  trans = transforms.ToPILImage(mode=\"RGB\")\n",
        "  video = trans(video)\n",
        "  plt.imshow(video)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyocvCWEmyf0"
      },
      "source": [
        "Build modello\n",
        "===="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxu-9BNV14E0"
      },
      "source": [
        "S3D\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jqQ29Sh15c2"
      },
      "source": [
        "class S3D(nn.Module):\n",
        "    def __init__(self, num_class):\n",
        "        super(S3D, self).__init__()\n",
        "        self.base = nn.Sequential(\n",
        "            SepConv3d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.MaxPool3d(kernel_size=(1,3,3), stride=(1,2,2), padding=(0,1,1)),\n",
        "            BasicConv3d(64, 64, kernel_size=1, stride=1),\n",
        "            SepConv3d(64, 192, kernel_size=3, stride=1, padding=1),\n",
        "            nn.MaxPool3d(kernel_size=(1,3,3), stride=(1,2,2), padding=(0,1,1)),\n",
        "            Mixed_3b(),\n",
        "            Mixed_3c(),\n",
        "            nn.MaxPool3d(kernel_size=(3,3,3), stride=(2,2,2), padding=(1,1,1)),\n",
        "            Mixed_4b(),\n",
        "            Mixed_4c(),\n",
        "            Mixed_4d(),\n",
        "            Mixed_4e(),\n",
        "            Mixed_4f(),\n",
        "            nn.MaxPool3d(kernel_size=(2,2,2), stride=(2,2,2), padding=(0,0,0)),\n",
        "            Mixed_5b(),\n",
        "            Mixed_5c(),\n",
        "        )\n",
        "        self.fc = nn.Sequential(nn.Conv3d(1024, num_class, kernel_size=1, stride=1, bias=True),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.base(x)\n",
        "        y = F.avg_pool3d(y, (2, y.size(3), y.size(4)), stride=1)\n",
        "        y = self.fc(y)\n",
        "        y = y.view(y.size(0), y.size(1), y.size(2))\n",
        "        logits = torch.mean(y, 2)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class BasicConv3d(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n",
        "        super(BasicConv3d, self).__init__()\n",
        "        self.conv = nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm3d(out_planes, eps=1e-3, momentum=0.001, affine=True)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class SepConv3d(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n",
        "        super(SepConv3d, self).__init__()\n",
        "        self.conv_s = nn.Conv3d(in_planes, out_planes, kernel_size=(1,kernel_size,kernel_size), stride=(1,stride,stride), padding=(0,padding,padding), bias=False)\n",
        "        self.bn_s = nn.BatchNorm3d(out_planes, eps=1e-3, momentum=0.001, affine=True)\n",
        "        self.relu_s = nn.ReLU()\n",
        "\n",
        "        self.conv_t = nn.Conv3d(out_planes, out_planes, kernel_size=(kernel_size,1,1), stride=(stride,1,1), padding=(padding,0,0), bias=False)\n",
        "        self.bn_t = nn.BatchNorm3d(out_planes, eps=1e-3, momentum=0.001, affine=True)\n",
        "        self.relu_t = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_s(x)\n",
        "        x = self.bn_s(x)\n",
        "        x = self.relu_s(x)\n",
        "\n",
        "        x = self.conv_t(x)\n",
        "        x = self.bn_t(x)\n",
        "        x = self.relu_t(x)\n",
        "        return x\n",
        "\n",
        "class Mixed_3b(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mixed_3b, self).__init__()\n",
        "\n",
        "        self.branch0 = nn.Sequential(\n",
        "            BasicConv3d(192, 64, kernel_size=1, stride=1),\n",
        "        )\n",
        "        self.branch1 = nn.Sequential(\n",
        "            BasicConv3d(192, 96, kernel_size=1, stride=1),\n",
        "            SepConv3d(96, 128, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            BasicConv3d(192, 16, kernel_size=1, stride=1),\n",
        "            SepConv3d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            BasicConv3d(192, 32, kernel_size=1, stride=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.branch0(x)\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "        out = torch.cat((x0, x1, x2, x3), 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Mixed_3c(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mixed_3c, self).__init__()\n",
        "        self.branch0 = nn.Sequential(\n",
        "            BasicConv3d(256, 128, kernel_size=1, stride=1),\n",
        "        )\n",
        "        self.branch1 = nn.Sequential(\n",
        "            BasicConv3d(256, 128, kernel_size=1, stride=1),\n",
        "            SepConv3d(128, 192, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            BasicConv3d(256, 32, kernel_size=1, stride=1),\n",
        "            SepConv3d(32, 96, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            BasicConv3d(256, 64, kernel_size=1, stride=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.branch0(x)\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "        out = torch.cat((x0, x1, x2, x3), 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Mixed_4b(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mixed_4b, self).__init__()\n",
        "\n",
        "        self.branch0 = nn.Sequential(\n",
        "            BasicConv3d(480, 192, kernel_size=1, stride=1),\n",
        "        )\n",
        "        self.branch1 = nn.Sequential(\n",
        "            BasicConv3d(480, 96, kernel_size=1, stride=1),\n",
        "            SepConv3d(96, 208, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            BasicConv3d(480, 16, kernel_size=1, stride=1),\n",
        "            SepConv3d(16, 48, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            BasicConv3d(480, 64, kernel_size=1, stride=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.branch0(x)\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "        out = torch.cat((x0, x1, x2, x3), 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Mixed_4c(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mixed_4c, self).__init__()\n",
        "\n",
        "        self.branch0 = nn.Sequential(\n",
        "            BasicConv3d(512, 160, kernel_size=1, stride=1),\n",
        "        )\n",
        "        self.branch1 = nn.Sequential(\n",
        "            BasicConv3d(512, 112, kernel_size=1, stride=1),\n",
        "            SepConv3d(112, 224, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            BasicConv3d(512, 24, kernel_size=1, stride=1),\n",
        "            SepConv3d(24, 64, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            BasicConv3d(512, 64, kernel_size=1, stride=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.branch0(x)\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "        out = torch.cat((x0, x1, x2, x3), 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Mixed_4d(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mixed_4d, self).__init__()\n",
        "\n",
        "        self.branch0 = nn.Sequential(\n",
        "            BasicConv3d(512, 128, kernel_size=1, stride=1),\n",
        "        )\n",
        "        self.branch1 = nn.Sequential(\n",
        "            BasicConv3d(512, 128, kernel_size=1, stride=1),\n",
        "            SepConv3d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            BasicConv3d(512, 24, kernel_size=1, stride=1),\n",
        "            SepConv3d(24, 64, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            BasicConv3d(512, 64, kernel_size=1, stride=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.branch0(x)\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "        out = torch.cat((x0, x1, x2, x3), 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Mixed_4e(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mixed_4e, self).__init__()\n",
        "\n",
        "        self.branch0 = nn.Sequential(\n",
        "            BasicConv3d(512, 112, kernel_size=1, stride=1),\n",
        "        )\n",
        "        self.branch1 = nn.Sequential(\n",
        "            BasicConv3d(512, 144, kernel_size=1, stride=1),\n",
        "            SepConv3d(144, 288, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            BasicConv3d(512, 32, kernel_size=1, stride=1),\n",
        "            SepConv3d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            BasicConv3d(512, 64, kernel_size=1, stride=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.branch0(x)\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "        out = torch.cat((x0, x1, x2, x3), 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Mixed_4f(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mixed_4f, self).__init__()\n",
        "\n",
        "        self.branch0 = nn.Sequential(\n",
        "            BasicConv3d(528, 256, kernel_size=1, stride=1),\n",
        "        )\n",
        "        self.branch1 = nn.Sequential(\n",
        "            BasicConv3d(528, 160, kernel_size=1, stride=1),\n",
        "            SepConv3d(160, 320, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            BasicConv3d(528, 32, kernel_size=1, stride=1),\n",
        "            SepConv3d(32, 128, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            BasicConv3d(528, 128, kernel_size=1, stride=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.branch0(x)\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "        out = torch.cat((x0, x1, x2, x3), 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Mixed_5b(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mixed_5b, self).__init__()\n",
        "\n",
        "        self.branch0 = nn.Sequential(\n",
        "            BasicConv3d(832, 256, kernel_size=1, stride=1),\n",
        "        )\n",
        "        self.branch1 = nn.Sequential(\n",
        "            BasicConv3d(832, 160, kernel_size=1, stride=1),\n",
        "            SepConv3d(160, 320, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            BasicConv3d(832, 32, kernel_size=1, stride=1),\n",
        "            SepConv3d(32, 128, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            BasicConv3d(832, 128, kernel_size=1, stride=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.branch0(x)\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "        out = torch.cat((x0, x1, x2, x3), 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Mixed_5c(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mixed_5c, self).__init__()\n",
        "\n",
        "        self.branch0 = nn.Sequential(\n",
        "            BasicConv3d(832, 384, kernel_size=1, stride=1),\n",
        "        )\n",
        "        self.branch1 = nn.Sequential(\n",
        "            BasicConv3d(832, 192, kernel_size=1, stride=1),\n",
        "            SepConv3d(192, 384, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            BasicConv3d(832, 48, kernel_size=1, stride=1),\n",
        "            SepConv3d(48, 128, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.MaxPool3d(kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            BasicConv3d(832, 128, kernel_size=1, stride=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.branch0(x)\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "        out = torch.cat((x0, x1, x2, x3), 1)\n",
        "        return out\n",
        "\n",
        "model = S3D(num_class=400)\n",
        "\n",
        "state_dict = torch.hub.load_state_dict_from_url(url=\"https://drive.google.com/uc?export=download&id=1HJVDBOQpnTMDVUM3SsXLy0HUkf_wryGO\")\n",
        "new_state_dict = collections.OrderedDict()\n",
        "for k,v in state_dict.items():\n",
        "  name = k[7:] # tolgo il module. dalla key\n",
        "  new_state_dict[name] = v\n",
        "model.load_state_dict(new_state_dict)\n",
        "\n",
        "model.fc = nn.Sequential(nn.Conv3d(1024, 10, kernel_size=1, stride=1, bias=True),)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpJRoMtT7jCH"
      },
      "source": [
        "Rete SlowFast - pre-trained\r\n",
        "--- \r\n",
        "https://github.com/facebookresearch/SlowFast/blob/master/MODEL_ZOO.md"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnS_Xs-V7wbt"
      },
      "source": [
        "cfg = get_cfg()\r\n",
        "cfg.merge_from_file('./slowfast/configs/Kinetics/c2/SLOWFAST_4x16_R50.yaml')\r\n",
        "url = 'https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/SLOWFAST_4x16_R50.pkl'\r\n",
        "\r\n",
        "filename = wget.download(url)\r\n",
        "cfg.TRAIN.CHECKPOINT_FILE_PATH = url\r\n",
        "cfg.TRAIN.CHECKPOINT_FILE_PATH = filename + 'skdosjd'\r\n",
        "cfg.TRAIN.CHECKPOINT_EPOCH_RESET = True\r\n",
        "cfg.NUM_GPUS = 1\r\n",
        "cfg.TRAIN.BATCH_SIZE = BATCH_TRAIN_SIZE\r\n",
        "cfg.DATA.NUM_FRAMES = 16\r\n",
        "model = build_model(cfg)\r\n",
        "r = load_checkpoint(filename,model,data_parallel=False,convert_from_caffe2=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1cr0T598VHp"
      },
      "source": [
        "for param in model.parameters():\r\n",
        "    param.requires_grad = False\r\n",
        "\r\n",
        "n_inputs = model.head.projection.in_features\r\n",
        "print(n_inputs)\r\n",
        "model.head.projection = nn.Linear(n_inputs, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVIiOuI_afG4"
      },
      "source": [
        "Rete SlowFast\r\n",
        "----\r\n",
        "\r\n",
        "https://github.com/r1ch88/SlowFastNetworks/blob/master/lib/slowfastnet.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWcB_XNyadot"
      },
      "source": [
        "_all__ = ['resnet50', 'resnet101','resnet152', 'resnet200']\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class Bottleneck(nn.Module):\r\n",
        "    expansion = 4\r\n",
        "\r\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, head_conv=1):\r\n",
        "        super(Bottleneck, self).__init__()\r\n",
        "        if head_conv == 1:\r\n",
        "            self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\r\n",
        "            self.bn1 = nn.BatchNorm3d(planes)\r\n",
        "        elif head_conv == 3:\r\n",
        "            self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=(3, 1, 1), bias=False, padding=(1, 0, 0))\r\n",
        "            self.bn1 = nn.BatchNorm3d(planes)\r\n",
        "        else:\r\n",
        "            raise ValueError(\"Unsupported head_conv!\")\r\n",
        "        self.conv2 = nn.Conv3d(\r\n",
        "            planes, planes, kernel_size=(1, 3, 3), stride=(1,stride,stride), padding=(0, 1, 1), bias=False)\r\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\r\n",
        "        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\r\n",
        "        self.bn3 = nn.BatchNorm3d(planes * 4)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.downsample = downsample\r\n",
        "        self.stride = stride\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        residual = x\r\n",
        "\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.bn1(out)\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        out = self.conv2(out)\r\n",
        "        out = self.bn2(out)\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        out = self.conv3(out)\r\n",
        "        out = self.bn3(out)\r\n",
        "\r\n",
        "        if self.downsample is not None:\r\n",
        "            residual = self.downsample(x)\r\n",
        "        out += residual\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class SlowFast(nn.Module):\r\n",
        "    def __init__(self, block=Bottleneck, layers=[3, 4, 6, 3], class_num=10, dropout=0.5 ):\r\n",
        "        super(SlowFast, self).__init__()\r\n",
        "\r\n",
        "        self.fast_inplanes = 8\r\n",
        "        self.fast_conv1 = nn.Conv3d(3, 8, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)\r\n",
        "        self.fast_bn1 = nn.BatchNorm3d(8)\r\n",
        "        self.fast_relu = nn.ReLU(inplace=True)\r\n",
        "        self.fast_maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\r\n",
        "        self.fast_res2 = self._make_layer_fast(block, 8, layers[0], head_conv=3)\r\n",
        "        self.fast_res3 = self._make_layer_fast(\r\n",
        "            block, 16, layers[1], stride=2, head_conv=3)\r\n",
        "        self.fast_res4 = self._make_layer_fast(\r\n",
        "            block, 32, layers[2], stride=2, head_conv=3)\r\n",
        "        self.fast_res5 = self._make_layer_fast(\r\n",
        "            block, 64, layers[3], stride=2, head_conv=3)\r\n",
        "        \r\n",
        "        self.lateral_p1 = nn.Conv3d(8, 8*2, kernel_size=(5, 1, 1), stride=(8, 1 ,1), bias=False, padding=(2, 0, 0))\r\n",
        "        self.lateral_res2 = nn.Conv3d(32,32*2, kernel_size=(5, 1, 1), stride=(8, 1 ,1), bias=False, padding=(2, 0, 0))\r\n",
        "        self.lateral_res3 = nn.Conv3d(64,64*2, kernel_size=(5, 1, 1), stride=(8, 1 ,1), bias=False, padding=(2, 0, 0))\r\n",
        "        self.lateral_res4 = nn.Conv3d(128,128*2, kernel_size=(5, 1, 1), stride=(8, 1 ,1), bias=False, padding=(2, 0, 0))\r\n",
        "\r\n",
        "        self.slow_inplanes = 64+64//8*2\r\n",
        "        self.slow_conv1 = nn.Conv3d(3, 64, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False)\r\n",
        "        self.slow_bn1 = nn.BatchNorm3d(64)\r\n",
        "        self.slow_relu = nn.ReLU(inplace=True)\r\n",
        "        self.slow_maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\r\n",
        "        self.slow_res2 = self._make_layer_slow(block, 64, layers[0], head_conv=1)\r\n",
        "        self.slow_res3 = self._make_layer_slow(\r\n",
        "            block, 128, layers[1], stride=2, head_conv=1)\r\n",
        "        self.slow_res4 = self._make_layer_slow(\r\n",
        "            block, 256, layers[2], stride=2, head_conv=3)\r\n",
        "        self.slow_res5 = self._make_layer_slow(\r\n",
        "            block, 512, layers[3], stride=2, head_conv=3)\r\n",
        "        \r\n",
        "        self.dp = nn.Dropout(dropout)\r\n",
        "        self.fc = nn.Linear(self.fast_inplanes+2048, class_num, bias=False)\r\n",
        "    \r\n",
        "    def forward(self, input ):\r\n",
        "        fast, lateral = self.FastPath(input[:, :, ::1, :, :])\r\n",
        "        slow = self.SlowPath(input[:, :, ::8, :, :], lateral)\r\n",
        "        x = torch.cat([slow, fast], dim=1)\r\n",
        "        x = self.dp(x)\r\n",
        "        x = self.fc(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def SlowPath(self, input, lateral):\r\n",
        "        x = self.slow_conv1(input)\r\n",
        "        x = self.slow_bn1(x)\r\n",
        "        x = self.slow_relu(x)\r\n",
        "        x = self.slow_maxpool(x)\r\n",
        "        x = torch.cat([x, lateral[0]],dim=1)\r\n",
        "        x = self.slow_res2(x)\r\n",
        "        x = torch.cat([x, lateral[1]],dim=1)\r\n",
        "        x = self.slow_res3(x)\r\n",
        "        x = torch.cat([x, lateral[2]],dim=1)\r\n",
        "        x = self.slow_res4(x)\r\n",
        "        x = torch.cat([x, lateral[3]],dim=1)\r\n",
        "        x = self.slow_res5(x)\r\n",
        "        x = nn.AdaptiveAvgPool3d(1)(x)\r\n",
        "        x = x.view(-1, x.size(1))\r\n",
        "        return x\r\n",
        "\r\n",
        "    def FastPath(self, input):\r\n",
        "        lateral = []\r\n",
        "        x = self.fast_conv1(input)\r\n",
        "        x = self.fast_bn1(x)\r\n",
        "        x = self.fast_relu(x)\r\n",
        "        pool1 = self.fast_maxpool(x)\r\n",
        "        lateral_p = self.lateral_p1(pool1)\r\n",
        "        lateral.append(lateral_p)\r\n",
        "\r\n",
        "        res2 = self.fast_res2(pool1)\r\n",
        "        lateral_res2 = self.lateral_res2(res2)\r\n",
        "        lateral.append(lateral_res2)\r\n",
        "        \r\n",
        "        res3 = self.fast_res3(res2)\r\n",
        "        lateral_res3 = self.lateral_res3(res3)\r\n",
        "        lateral.append(lateral_res3)\r\n",
        "\r\n",
        "        res4 = self.fast_res4(res3)\r\n",
        "        lateral_res4 = self.lateral_res4(res4)\r\n",
        "        lateral.append(lateral_res4)\r\n",
        "\r\n",
        "        res5 = self.fast_res5(res4)\r\n",
        "        x = nn.AdaptiveAvgPool3d(1)(res5)\r\n",
        "        x = x.view(-1, x.size(1))\r\n",
        "\r\n",
        "        return x, lateral\r\n",
        "\r\n",
        "    def _make_layer_fast(self, block, planes, blocks, stride=1, head_conv=1):\r\n",
        "        downsample = None\r\n",
        "        if stride != 1 or self.fast_inplanes != planes * block.expansion:\r\n",
        "            downsample = nn.Sequential(\r\n",
        "                nn.Conv3d(\r\n",
        "                    self.fast_inplanes,\r\n",
        "                    planes * block.expansion,\r\n",
        "                    kernel_size=1,\r\n",
        "                    stride=(1,stride,stride),\r\n",
        "                    bias=False), nn.BatchNorm3d(planes * block.expansion))\r\n",
        "\r\n",
        "        layers = []\r\n",
        "        layers.append(block(self.fast_inplanes, planes, stride, downsample, head_conv=head_conv))\r\n",
        "        self.fast_inplanes = planes * block.expansion\r\n",
        "        for i in range(1, blocks):\r\n",
        "            layers.append(block(self.fast_inplanes, planes, head_conv=head_conv))\r\n",
        "        return nn.Sequential(*layers)\r\n",
        "\r\n",
        "    def _make_layer_slow(self, block, planes, blocks, stride=1, head_conv=1):\r\n",
        "        downsample = None\r\n",
        "        if stride != 1 or self.slow_inplanes != planes * block.expansion:\r\n",
        "            downsample = nn.Sequential(\r\n",
        "                nn.Conv3d(\r\n",
        "                    self.slow_inplanes,\r\n",
        "                    planes * block.expansion,\r\n",
        "                    kernel_size=1,\r\n",
        "                    stride=(1,stride,stride),\r\n",
        "                    bias=False), nn.BatchNorm3d(planes * block.expansion))\r\n",
        "\r\n",
        "        layers = []\r\n",
        "        layers.append(block(self.slow_inplanes, planes, stride, downsample, head_conv=head_conv))\r\n",
        "        self.slow_inplanes = planes * block.expansion\r\n",
        "        for i in range(1, blocks):\r\n",
        "            layers.append(block(self.slow_inplanes, planes, head_conv=head_conv))\r\n",
        "  \r\n",
        "        self.slow_inplanes = planes * block.expansion + planes * block.expansion//8*2\r\n",
        "        return nn.Sequential(*layers)\r\n",
        "\r\n",
        "\r\n",
        "def resnet18(**kwargs):\r\n",
        "    \"\"\"Constructs a ResNet-18 model.\r\n",
        "    \"\"\"\r\n",
        "    model = SlowFast(Bottleneck, [2, 2, 2, 2], **kwargs)\r\n",
        "    return model\r\n",
        "\r\n",
        "def resnet50(**kwargs):\r\n",
        "    \"\"\"Constructs a ResNet-50 model.\r\n",
        "    \"\"\"\r\n",
        "    model = SlowFast(Bottleneck, [3, 4, 6, 3], **kwargs)\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "def resnet101(**kwargs):\r\n",
        "    \"\"\"Constructs a ResNet-101 model.\r\n",
        "    \"\"\"\r\n",
        "    model = SlowFast(Bottleneck, [3, 4, 23, 3], **kwargs)\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "def resnet152(**kwargs):\r\n",
        "    \"\"\"Constructs a ResNet-101 model.\r\n",
        "    \"\"\"\r\n",
        "    model = SlowFast(Bottleneck, [3, 8, 36, 3], **kwargs)\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "def resnet200(**kwargs):\r\n",
        "    \"\"\"Constructs a ResNet-101 model.\r\n",
        "    \"\"\"\r\n",
        "    model = SlowFast(Bottleneck, [3, 24, 36, 3], **kwargs)\r\n",
        "    return model\r\n",
        "\r\n",
        "model = resnet50()\r\n",
        "n_inputs = model.fc.in_features\r\n",
        "model.fc = nn.Linear(n_inputs, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKpTDWtyorUz"
      },
      "source": [
        "Training modello\n",
        "===="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfNX6tByotkB"
      },
      "source": [
        "def train_one_epoch(model, optimizer, trainloader, epoch, num_epochs):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  train_correct = 0\n",
        "  \n",
        "  for batch_i,item in enumerate(trainloader):\n",
        "    video, target = item['video'], item['label']\n",
        "    target = torch.reshape(target, (-1,))\n",
        "    video, target = video.to(device), target.to(device)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(video)\n",
        "    loss = criterion(outputs, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    correct_lbls = (predicted == target).sum().item()\n",
        "    train_correct = train_correct + correct_lbls\n",
        "    running_loss = running_loss + loss.item()\n",
        "\n",
        "    # Print log\n",
        "    sys.stdout.write(\n",
        "      \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f), Acc: %.2f%% (%.2f%%)] [lr: %.7f%%]\"\n",
        "      % (\n",
        "        epoch+1,\n",
        "        num_epochs,\n",
        "        batch_i,\n",
        "        len(trainloader),\n",
        "        loss.item(),\n",
        "        running_loss/(1+batch_i),\n",
        "        100*(correct_lbls/float(BATCH_TRAIN_SIZE)),\n",
        "        100*(train_correct/float(BATCH_TRAIN_SIZE*(1+batch_i))),\n",
        "        optimizer.param_groups[0]['lr']\n",
        "      )\n",
        "    )\n",
        "\n",
        "\n",
        "  return [running_loss, train_correct]\n",
        "\n",
        "\n",
        "def evaluate(model, validloader, isTestingPhase):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  valid_correct = 0\n",
        "  if isTestingPhase:\n",
        "    predicts, labels = [],[]\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for item in validloader:\n",
        "      video, target = item['video'], item['label']\n",
        "      target = torch.reshape(target, (-1,))\n",
        "      video, target = video.to(device), target.to(device)\n",
        "\n",
        "\n",
        "      # INFERENCE CALCULATING\n",
        "      outputs = model(video)\n",
        "      val_loss = criterion(outputs, target)\n",
        "\n",
        "      # MONITORING ACCURACY AND LOSS      \n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      valid_correct = valid_correct + (predicted == target).sum().item()\n",
        "      running_loss += val_loss.item()\n",
        "\n",
        "      if isTestingPhase:\n",
        "        predicts.append(predicted.data)\n",
        "        labels.append(target.data)\n",
        "\n",
        "  if isTestingPhase:\n",
        "    return [(predicts, labels), running_loss, valid_correct]\n",
        "  \n",
        "  else:\n",
        "    return [None, running_loss, valid_correct]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRwSz0Y0p_oM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "671b0da9-eb99-40c6-b8d2-9fcc5345051d"
      },
      "source": [
        "%ls drive/MyDrive/*.pth"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/MyDrive/r2p1d50_KM_200ep.pth\n",
            "drive/MyDrive/_SlowFast_Adam_lr1.0000000000000004e-08_e27_1614825224.pth\n",
            "drive/MyDrive/_SlowFast_Adam_lr1e-05_e22_1614821137.pth\n",
            "drive/MyDrive/_SlowFast_SGD_lr1.0000000000000006e-11_e40_1614821617.pth\n",
            "drive/MyDrive/_VideoResNet_SGD_lr0.001_e0_1614936970.pth\n",
            "drive/MyDrive/_VideoResNet_SGD_lr0.001_e1_1614941395.pth\n",
            "drive/MyDrive/_VideoResNet_SGD_lr0.001_e2_1614944394.pth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHy3wyv9owPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f372b59a-3105-40a8-d610-f46733e9cb53"
      },
      "source": [
        "# SVUOTO LA CACHE\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# COSTRUISCO OPTIMIZER: learning rate, optimizer e scheluder lr \n",
        "lr = 1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
        "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) # classico\n",
        "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1, min_lr=1e-6) # varia lr su plateau\n",
        "\n",
        "# COSTRUISCO CRITERIO DI LOSS: pesi per ds sbilanciato e criterion\n",
        "y_train = [v for k,v in train_set]\n",
        "weights = torch.FloatTensor(compute_class_weight('balanced', np.unique(y_train), y_train )).cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# DEFINISCO PARAMETRI DI TRAINING E VALIDATION\n",
        "starting_epoch = 0 # numero epoca di partenza\n",
        "EPOCHS=70\n",
        "tr_loss, val_loss = [], []\n",
        "tr_accs, val_accs = [], []\n",
        "\n",
        "# RECUPERO EPOCHE SALVATE\n",
        "restore = False\n",
        "if(restore):\n",
        "  checkpoint_init = torch.load('drive/MyDrive/Copia di _SlowFast_SGD_lr0.001_e12_1614555421.pth')\n",
        "  starting_epoch = checkpoint_init['epoch'] + 1 \n",
        "  model.load_state_dict(checkpoint_init['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint_init['optimizer_state_dict'])\n",
        "  criterion = checkpoint_init['loss']\n",
        "  lr_scheduler.load_state_dict(checkpoint_init['scheduler'])\n",
        "  tr_loss, val_loss = checkpoint_init['losses']\n",
        "  tr_accs, val_accs = checkpoint_init['accs']\n",
        "\n",
        "print()\n",
        "print(\"val_set_size: \" + str(test_size), end =\" + \")\n",
        "print(str(new_W)+\"x\"+str(new_H), end =\" + \")\n",
        "print(model.__class__.__name__, end =\" + \")\n",
        "print(optimizer.param_groups[0]['lr'], end =\" + \")\n",
        "print(optimizer.__class__.__name__, end =\" + \")\n",
        "print(lr_scheduler.__class__.__name__, end =\" + \")\n",
        "print(criterion.__class__.__name__)\n",
        "if not restore: \n",
        "  print(\" + \" + \"from scratch!\")\n",
        "else:\n",
        "  print(\" + \" + \"not from scratch!\")\n",
        "\n",
        "# INIZIO ADDESTRAMENTO\n",
        "model = model.to(device)\n",
        "for epoch in range(starting_epoch, EPOCHS):\n",
        "  # TRAIN FOR ONE EPOCH\n",
        "  start_time = time.time()\n",
        "  train_loss, train_correct = train_one_epoch(model, optimizer, trainLoader, epoch, EPOCHS)\n",
        "  \n",
        "  # VALIDATE RESULT\n",
        "  _, valid_loss, valid_correct = evaluate(model, validLoader, isTestingPhase=False)\n",
        "\n",
        "  # STEP TO APPLY LEARNING SCHEDULER\n",
        "  #lr_scheduler.step() \n",
        "  lr_scheduler.step( valid_loss/float(len(validLoader)) )\n",
        "\n",
        "  print(\n",
        "      f' [Val_loss = {valid_loss/float(len(validLoader)):0.7f},'\n",
        "      f' Val_acc = {100*valid_correct/float(len(validLoader)*BATCH_TEST_SIZE):0.7f}]',\n",
        "      f'in {time.time()-start_time:.2f} sec'\n",
        "  )\n",
        "\n",
        "  tr_loss.append(train_loss/float(len(trainLoader)))\n",
        "  val_loss.append(valid_loss/float(len(validLoader)))\n",
        "  tr_accs.append(float(train_correct)/float(len(trainLoader)*BATCH_TRAIN_SIZE))\n",
        "  val_accs.append(float(valid_correct)/float(len(validLoader)*BATCH_TEST_SIZE))\n",
        "\n",
        "  checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': criterion,\n",
        "            'scheduler':lr_scheduler.state_dict(),\n",
        "            'losses': (tr_loss, val_loss),\n",
        "            'accs': (tr_accs, val_accs)\n",
        "  }\n",
        "\n",
        "  path=\"drive/MyDrive/\"\n",
        "  lr_ = optimizer.param_groups[0]['lr']\n",
        "  t = str(round(time.time()))\n",
        "  torch.save(checkpoint, path+'_'+model.__class__.__name__+'_'+optimizer.__class__.__name__+'_lr'+str(lr_)+'_e'+str(epoch)+'_'+t+'.pth')\n",
        "  print(t,end=\"\")\n",
        "\n",
        "plt.plot(tr_loss, label='Training loss', c='r')\n",
        "plt.plot(val_loss, label='Validation loss', c='b')\n",
        "plt.legend(frameon=False)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(tr_accs, label='Training accuracy', c='r')\n",
        "plt.plot(val_accs, label='Validation accuracy', c='b')\n",
        "plt.legend(frameon=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_set_size: 0.2 + 224x150 + S3D + 0.001 + Adam + ReduceLROnPlateau + CrossEntropyLoss\n",
            " + from scratch!\n",
            "[Epoch 1/70] [Batch 1297/1298] [Loss: 0.257306 (1.016488), Acc: 87.50% (64.61%)] [lr: 0.0010000%] [Val_loss = 0.8388215, Val_acc = 69.4423077] in 785.02 sec\n",
            "[Epoch 2/70] [Batch 1297/1298] [Loss: 0.527666 (0.739712), Acc: 75.00% (73.75%)] [lr: 0.0010000%] [Val_loss = 0.7854354, Val_acc = 73.4807692] in 784.72 sec\n",
            "[Epoch 3/70] [Batch 1297/1298] [Loss: 0.399877 (0.606895), Acc: 75.00% (78.04%)] [lr: 0.0010000%] [Val_loss = 0.6909008, Val_acc = 75.6538462] in 782.58 sec\n",
            "[Epoch 4/70] [Batch 1297/1298] [Loss: 0.289706 (0.516741), Acc: 87.50% (81.18%)] [lr: 0.0010000%] [Val_loss = 0.6602139, Val_acc = 75.7307692] in 781.11 sec\n",
            "[Epoch 5/70] [Batch 1297/1298] [Loss: 0.885269 (0.448632), Acc: 62.50% (83.88%)] [lr: 0.0010000%] [Val_loss = 0.7175327, Val_acc = 76.3076923] in 780.27 sec\n",
            "[Epoch 6/70] [Batch 1297/1298] [Loss: 0.632341 (0.393977), Acc: 81.25% (85.68%)] [lr: 0.0010000%] [Val_loss = 0.7220416, Val_acc = 75.2692308] in 783.45 sec\n",
            "[Epoch 7/70] [Batch 1297/1298] [Loss: 0.557964 (0.353440), Acc: 68.75% (86.82%)] [lr: 0.0010000%] [Val_loss = 0.7996967, Val_acc = 74.0961538] in 787.78 sec\n",
            "[Epoch 8/70] [Batch 1297/1298] [Loss: 0.637989 (0.306915), Acc: 75.00% (88.63%)] [lr: 0.0010000%] [Val_loss = 0.6510869, Val_acc = 78.9615385] in 783.84 sec\n",
            "[Epoch 9/70] [Batch 1297/1298] [Loss: 0.214985 (0.280928), Acc: 87.50% (89.55%)] [lr: 0.0010000%] [Val_loss = 0.6601949, Val_acc = 79.4807692] in 786.05 sec\n",
            "[Epoch 10/70] [Batch 1297/1298] [Loss: 0.600535 (0.247612), Acc: 75.00% (90.47%)] [lr: 0.0010000%] [Val_loss = 0.9207411, Val_acc = 73.5000000] in 798.30 sec\n",
            "[Epoch 11/70] [Batch 1297/1298] [Loss: 0.520098 (0.231369), Acc: 87.50% (91.54%)] [lr: 0.0010000%] [Val_loss = 0.7287442, Val_acc = 78.7307692] in 799.34 sec\n",
            "[Epoch 12/70] [Batch 1297/1298] [Loss: 0.053837 (0.209223), Acc: 93.75% (92.44%)] [lr: 0.0010000%] [Val_loss = 0.7662206, Val_acc = 79.2307692] in 790.14 sec\n",
            "[Epoch 13/70] [Batch 1297/1298] [Loss: 0.466104 (0.189590), Acc: 68.75% (92.90%)] [lr: 0.0010000%] [Val_loss = 0.7822277, Val_acc = 79.1538462] in 789.32 sec\n",
            "[Epoch 14/70] [Batch 1297/1298] [Loss: 0.204933 (0.177244), Acc: 87.50% (93.52%)] [lr: 0.0010000%] [Val_loss = 0.8489345, Val_acc = 78.4615385] in 787.63 sec\n",
            "[Epoch 15/70] [Batch 1297/1298] [Loss: 0.075778 (0.111336), Acc: 93.75% (95.98%)] [lr: 0.0001000%] [Val_loss = 0.7164618, Val_acc = 82.3846154] in 789.24 sec\n",
            "[Epoch 16/70] [Batch 1297/1298] [Loss: 0.010012 (0.090988), Acc: 93.75% (96.71%)] [lr: 0.0001000%] [Val_loss = 0.7199443, Val_acc = 82.4615385] in 791.85 sec\n",
            "[Epoch 17/70] [Batch 1297/1298] [Loss: 0.331427 (0.075292), Acc: 87.50% (97.37%)] [lr: 0.0001000%] [Val_loss = 0.7586143, Val_acc = 81.9423077] in 800.02 sec\n",
            "[Epoch 18/70] [Batch 1297/1298] [Loss: 0.200466 (0.062831), Acc: 87.50% (97.77%)] [lr: 0.0001000%] [Val_loss = 0.7736165, Val_acc = 82.7500000] in 801.72 sec\n",
            "[Epoch 19/70] [Batch 1297/1298] [Loss: 0.002867 (0.064426), Acc: 93.75% (97.63%)] [lr: 0.0001000%] [Val_loss = 0.8069397, Val_acc = 81.7692308] in 796.74 sec\n",
            "[Epoch 20/70] [Batch 1297/1298] [Loss: 0.026886 (0.057601), Acc: 93.75% (97.99%)] [lr: 0.0001000%] [Val_loss = 0.8082709, Val_acc = 82.4423077] in 797.40 sec\n",
            "[Epoch 21/70] [Batch 1297/1298] [Loss: 0.030906 (0.049655), Acc: 93.75% (98.36%)] [lr: 0.0000100%] [Val_loss = 0.8116101, Val_acc = 82.4807692] in 801.73 sec\n",
            "[Epoch 22/70] [Batch 1297/1298] [Loss: 0.084470 (0.046565), Acc: 87.50% (98.35%)] [lr: 0.0000100%] [Val_loss = 0.8243646, Val_acc = 82.4230769] in 802.86 sec\n",
            "[Epoch 23/70] [Batch 1297/1298] [Loss: 0.006327 (0.046020), Acc: 93.75% (98.34%)] [lr: 0.0000100%] [Val_loss = 0.8278302, Val_acc = 82.5000000] in 797.96 sec\n",
            "[Epoch 24/70] [Batch 1297/1298] [Loss: 0.002630 (0.046919), Acc: 93.75% (98.37%)] [lr: 0.0000100%] [Val_loss = 0.8349666, Val_acc = 82.5000000] in 798.50 sec\n",
            "[Epoch 25/70] [Batch 1297/1298] [Loss: 0.006578 (0.044189), Acc: 93.75% (98.56%)] [lr: 0.0000100%] [Val_loss = 0.8373535, Val_acc = 82.6346154] in 787.51 sec\n",
            "[Epoch 26/70] [Batch 1297/1298] [Loss: 0.012525 (0.044031), Acc: 93.75% (98.56%)] [lr: 0.0000100%] [Val_loss = 0.8395990, Val_acc = 82.6923077] in 785.58 sec\n",
            "[Epoch 27/70] [Batch 1297/1298] [Loss: 0.269697 (0.044603), Acc: 81.25% (98.42%)] [lr: 0.0000010%] [Val_loss = 0.8414484, Val_acc = 82.5769231] in 783.82 sec\n",
            "[Epoch 28/70] [Batch 1297/1298] [Loss: 0.009297 (0.047871), Acc: 93.75% (98.37%)] [lr: 0.0000010%] [Val_loss = 0.8424579, Val_acc = 82.6538462] in 791.61 sec\n",
            "[Epoch 29/70] [Batch 1297/1298] [Loss: 0.013736 (0.043936), Acc: 93.75% (98.47%)] [lr: 0.0000010%] [Val_loss = 0.8405951, Val_acc = 82.5961538] in 787.85 sec\n",
            "[Epoch 30/70] [Batch 1297/1298] [Loss: 0.070070 (0.044758), Acc: 87.50% (98.47%)] [lr: 0.0000010%] [Val_loss = 0.8413458, Val_acc = 82.5192308] in 788.01 sec\n",
            "[Epoch 31/70] [Batch 1297/1298] [Loss: 0.016753 (0.043163), Acc: 93.75% (98.47%)] [lr: 0.0000010%] [Val_loss = 0.8429786, Val_acc = 82.5384615] in 794.47 sec\n",
            "[Epoch 32/70] [Batch 1297/1298] [Loss: 0.009823 (0.048811), Acc: 93.75% (98.24%)] [lr: 0.0000010%] [Val_loss = 0.8425945, Val_acc = 82.5384615] in 797.79 sec\n",
            "[Epoch 33/70] [Batch 1297/1298] [Loss: 0.006788 (0.045368), Acc: 93.75% (98.40%)] [lr: 0.0000010%] [Val_loss = 0.8416871, Val_acc = 82.5961538] in 794.22 sec\n",
            "[Epoch 34/70] [Batch 1297/1298] [Loss: 0.061277 (0.044491), Acc: 87.50% (98.38%)] [lr: 0.0000010%] [Val_loss = 0.8446710, Val_acc = 82.6538462] in 791.22 sec\n",
            "[Epoch 35/70] [Batch 1297/1298] [Loss: 0.013079 (0.044960), Acc: 93.75% (98.41%)] [lr: 0.0000010%] [Val_loss = 0.8420167, Val_acc = 82.5961538] in 791.23 sec\n",
            "[Epoch 36/70] [Batch 1297/1298] [Loss: 0.005367 (0.044410), Acc: 93.75% (98.46%)] [lr: 0.0000010%] [Val_loss = 0.8427430, Val_acc = 82.5961538] in 799.65 sec\n",
            "[Epoch 37/70] [Batch 1297/1298] [Loss: 0.024991 (0.039842), Acc: 93.75% (98.56%)] [lr: 0.0000010%] [Val_loss = 0.8420108, Val_acc = 82.5192308] in 798.23 sec\n",
            "[Epoch 38/70] [Batch 1297/1298] [Loss: 0.008518 (0.048313), Acc: 93.75% (98.35%)] [lr: 0.0000010%] [Val_loss = 0.8422749, Val_acc = 82.5384615] in 802.41 sec\n",
            "[Epoch 39/70] [Batch 1297/1298] [Loss: 0.012776 (0.044464), Acc: 93.75% (98.49%)] [lr: 0.0000010%] [Val_loss = 0.8434477, Val_acc = 82.6346154] in 802.63 sec\n",
            "[Epoch 40/70] [Batch 1297/1298] [Loss: 0.000292 (0.042830), Acc: 93.75% (98.54%)] [lr: 0.0000010%] [Val_loss = 0.8432799, Val_acc = 82.6153846] in 804.85 sec\n",
            "[Epoch 41/70] [Batch 1297/1298] [Loss: 0.016485 (0.046312), Acc: 93.75% (98.33%)] [lr: 0.0000010%] [Val_loss = 0.8428573, Val_acc = 82.6153846] in 809.33 sec\n",
            "[Epoch 42/70] [Batch 1297/1298] [Loss: 0.029051 (0.043927), Acc: 93.75% (98.48%)] [lr: 0.0000010%] [Val_loss = 0.8425928, Val_acc = 82.5961538] in 803.06 sec\n",
            "[Epoch 43/70] [Batch 1297/1298] [Loss: 0.002212 (0.041735), Acc: 93.75% (98.58%)] [lr: 0.0000010%] [Val_loss = 0.8452177, Val_acc = 82.6346154] in 792.92 sec\n",
            "[Epoch 44/70] [Batch 1297/1298] [Loss: 0.069424 (0.043004), Acc: 87.50% (98.57%)] [lr: 0.0000010%] [Val_loss = 0.8423873, Val_acc = 82.6153846] in 786.15 sec\n",
            "[Epoch 45/70] [Batch 596/1298] [Loss: 0.004462 (0.044688), Acc: 100.00% (98.45%)] [lr: 0.0000010%]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd5xj_h5MfBC"
      },
      "source": [
        "Testing\n",
        "==="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N38xzqlOTNaR"
      },
      "source": [
        "model = S3D(num_class=10) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqEO58X5NLP9"
      },
      "source": [
        "checkpoint_init = torch.load('drive/MyDrive/_S3D_Adam_lr1.0000000000000002e-06_e28_1615043011.pth')\r\n",
        "model.load_state_dict(checkpoint_init['model_state_dict'])\r\n",
        "model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUrSjxbFMhyc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f278eed-67b6-4bd6-e6e1-d7eefd70d616"
      },
      "source": [
        "criterion  = nn.CrossEntropyLoss(weight=weights)\n",
        "outputTargetTuple, test_loss, test_correct = evaluate(model, testLoader, isTestingPhase=True)\n",
        "y_pred, y_true = torch.cat(outputTargetTuple[0]).cpu().detach().numpy(), torch.cat(outputTargetTuple[1]).cpu().detach().numpy() \n",
        "print(f'Accuracy = {(test_correct/float(len(testLoader)*BATCH_TEST_SIZE))*100}%')\n",
        "print(classification_report(y_true=y_true, y_pred=y_pred, zero_division=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 81.36673850574712%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.85      0.86       292\n",
            "           1       0.83      0.73      0.78       318\n",
            "           2       0.91      0.82      0.86      1841\n",
            "           3       0.94      0.94      0.94      1068\n",
            "           4       0.87      0.83      0.85       125\n",
            "           5       0.85      0.88      0.86       709\n",
            "           6       0.73      0.76      0.74      1178\n",
            "           7       0.82      0.79      0.81       228\n",
            "           8       0.75      0.78      0.76      1916\n",
            "           9       0.78      0.81      0.80      3451\n",
            "\n",
            "    accuracy                           0.81     11126\n",
            "   macro avg       0.84      0.82      0.83     11126\n",
            "weighted avg       0.82      0.81      0.82     11126\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}